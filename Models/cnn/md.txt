class CNNM(nn.Module):
    def __init__(self):
        super().__init__()
        self.mp = nn.MaxPool2d(2)
        self.flt = nn.Flatten()
        self.ln = nn.LayerNorm((1, 128, 400))
        self.b1 = nn.BatchNorm2d(1)
        self.c1 = nn.Conv2d(in_channels = 1, out_channels = 9, kernel_size = 3) #126 398 -> 63 199
        self.b2 = nn.BatchNorm2d(9)
        self.c2 = nn.Conv2d(in_channels = 9, out_channels = 18, kernel_size = 2) #62 198 -> 31 99
        self.b3 = nn.BatchNorm2d(18)
        self.c3 = nn.Conv2d(in_channels = 18, out_channels = 18, kernel_size = 2) #30 -> 98 -> 15 49
        self.b4 = nn.BatchNorm2d(18)
        self.c4 = nn.Conv2d(in_channels = 18, out_channels = 9, kernel_size = 3, padding = 1) #
        self.c5 = nn.Conv2d(in_channels = 9, out_channels = 3, kernel_size = 3, padding = 1)
        self.c6 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 3, padding = 1)

        self.l1 = nn.Linear(15 * 49 * 3, 3000)
        self.l2 = nn.Linear(3000, 3000)
        self.l3 = nn.Linear(3000, 500)
        self.l4 = nn.Linear(500, 100)
        self.l5 = nn.Linear(100, 15)
        self.l6 = nn.Linear(15, 2)
        self.f1 = nn.Softmax(dim = 1)

    def forward(self, x):
        x = -x
        x = x.reshape(x.shape[0], 1, 128, 400)
        x = self.b1(x)
        x = self.ln(x)
        x = F.relu(self.c1(x))
        x = self.mp(x)
        x = self.b2(x)
        x = F.relu(self.c2(x))
        x = self.mp(x)
        x = self.b3(x)
        x = F.relu(self.c3(x))
        x = self.mp(x)
        x = self.b4(x)
        x = F.relu(self.c4(x))
        x = F.relu(self.c5(x))
        x = F.relu(self.c6(x))
        x = self.flt(x)
        x = F.relu(self.l1(x))
        x = F.relu(self.l2(x))
        x = F.relu(self.l3(x))
        x = F.relu(self.l4(x))
        x = F.relu(self.l5(x))
        x = F.relu(self.l6(x))
        x = self.f1(x)

        return x