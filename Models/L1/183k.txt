class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.flt = nn.Flatten()
        self.b1 = nn.BatchNorm1d(128 * 400)
        self.l1 = nn.Linear(128 * 400, 100 * 100)
        self.d1 = nn.Dropout(p = 0.2)
        self.b2 = nn.BatchNorm1d(100 * 100)
        self.l2 = nn.Linear(100 * 100, 100 * 100)
        self.d2 = nn.Dropout(p = 0.2)
        #self.b1 = nn.BatchMorm1d
        self.l3 = nn.Linear(100 * 100, 100 * 100)
        self.d3 = nn.Dropout()
        self.l4 = nn.Linear(100 * 100, 128 * 20)
        self.d4 = nn.Dropout()
        self.l5 = nn.Linear(128 * 20, 80 * 40)
        self.d5 = nn.Dropout()
        self.l6 = nn.Linear(80 * 40, 40 * 10)
        self.d6 = nn.Dropout()
        self.l7 = nn.Linear(400, 20)
        self.d7 = nn.Dropout()
        self.l8 = nn.Linear(20, 1)
        self.d8 = nn.Dropout()
        self.f1 = nn.Softmax()
    def forward(self, x):
        x = self.flt(x)
        x = self.b1(x)
        x = F.relu(self.d1(self.l1(x)))
        x = self.b2(x)
        x = F.relu(self.d2(self.l2(x)))
        x = F.relu(self.d3(self.l3(x)))
        x = F.relu(self.d4(self.l4(x)))
        x = F.relu(self.d5(self.l5(x)))
        x = F.relu(self.d6(self.l6(x)))
        x = F.relu(self.d7(self.l7(x)))
        x = F.relu(self.d8(self.l8(x)))
        x = self.f1(x)
        return x